#!/usr/bin/env python

from __future__ import division

import sys
import os
import gc
import optparse
import tempfile
import shutil
import EXIF, time
import traceback

try:
    from sqlite3 import dbapi2 as sqlite
except ImportError:
    from pysqlite2 import dbapi2 as sqlite
from gtk import gdk
import gobject

from db_utils import shotwell_photo_id_to_db_id, shotwell_video_id_to_db_id

SHOTWELL_DB = os.path.expanduser("~/.shotwell/data/photo.db")
VERSION = 0.1
jhead_checked = False
jhead_available = True

options = None
cxn = sqlite.connect(SHOTWELL_DB)
output_path = None
loop = None

# square thumbnail size in pixels
THUMB_SIZE=100

def dbg(format, *args):
    if options.debug:
        fr = sys._getframe(1)
        filename = os.path.basename(fr.f_code.co_filename)
        func = fr.f_code.co_name
        lineno = fr.f_lineno
        sys.stderr.write(('%s:%s():%d: ' + format + '\n')
                         % ((filename, func, lineno) + args))

def info(format, *args):
    if not options.quiet:
        print format % args

def warn(format, *args):
    sys.stderr.write(('Warning: ' + format + '\n') % args)

def err(format, *args):
    sys.stderr.write(('Error: ' + format + '\n') % args)
    sys.exit(1)

def encode(s, charset='utf-8'):
    try:
        return s.encode(charset)
    except UnicodeError:
        return s.encode(charset, 'replace')


def create_video_export_table():
    cur = cxn.cursor()
    cur.execute("create table original_video_export ("
                "   id integer primary key not null, "
                "   thumbnail_path string not null,  "
                "   webm_video_path string not null  "
                ")")
    cxn.commit()

def ensure_original_in_shotwell():
    cur = cxn.cursor()
    try:
        cur.execute("select data from original_options where name='original_db_version'")
        version, = cur.fetchone()
    except:
        dbg('creating original_exports table in shotwell database')
        cur.execute("create table original_exports (     "
                    "   id integer primary key not null, "
                    "   normal_relpath string not null,  "
                    "   thumb_relpath string not null,   "
                    "   mq_relpath string not null,      "
                    "   hq_relpath string not null       "
                    ")")
        cur.execute("create table original_options (     "
                    "   name string primary key not null, "
                    "   data string"
                    ")")
        cxn.commit()
        cur.execute("insert into original_options (name, data) "
                    "values('original_db_version', '2')")
        cxn.commit()
        create_video_export_table()

    cur.execute("select data from original_options where name='original_db_version'")
    version, = cur.fetchone()
    version = int(version)
    if version < 2:
        create_video_export_table()
        cur.execute("update original_options set data='2' where name='original_db_version'")
        cxn.commit()

    if options.resync:
        cur.execute("delete from original_exports")
        cxn.commit()

def ensure_output_path_set():
    global output_path
    cur = cxn.cursor()
    cur.execute("select data from original_options where name='original_output_path'")
    try:
        uri, = cur.fetchone()
    except:
        uri = None
    if not uri:
        if not options.output_path:
            err('This appears to be the first time you run this program.\n'
                'You will need to use the --output-uri option to set the\n'
                'location of where you want your remote gallery to be.')
        cur.execute('insert into original_options (data,name) values(%r,%r)' %
                    (options.output_path, 'original_output_path'))
        cxn.commit()
    else:
        if options.output_path and options.output_path != uri:
            if not options.resync:
                err('Output URI %s different from the one you used before,\n'
                    '%s. Add the --resync option to force a resync.',
                    options.output_path, uri)
            info('Updating output URI from %s to %s', uri,
                 options.output_path)
            cur.execute('update original_options set data=%s where name=%s',
                        (options.output_path, 'original_output_path'))
            cxn.commit()

    cur.execute("select data from original_options where name='original_output_path'")
    output_path, = cur.fetchone()

    info('Checking that %s exists...', output_path)
    if not os.path.isdir(output_path):
        info('Output path does not exist, trying to create it...')
        os.makedirs(output_path)

def init(args):
    global options

    parser = optparse.OptionParser()
    parser.add_option('-d', '--debug',
                      action="store_true", dest="debug",
                      help="run in debugging mode")
    parser.add_option('-q', '--quiet',
                      action="store_true", dest="quiet",
                      help="be quiet")
    parser.add_option('', '--version',
                      action="store_true", dest="version",
                      help="show version information")
    parser.add_option('', '--photos-path',
                      action="store", type="string", dest="photos_path",
                      default=os.path.expanduser("~/Photos"),
                      help="path to the user's photos (default ~/Photos)")
    parser.add_option('', '--output-path',
                      action="store", type="string", dest="output_path",
                      help=("where to store the scaled photos"))
    parser.add_option('', '--resync',
                      action="store_true", dest="resync",
                      help=("resync all photos. necessary if you change "
                            "the output URI, or want your old photos at "
                            "a different size."))
    parser.add_option('-s', '--scaled-size',
                      action="store", type="int", dest="scaled_size",
                      default=500,
                      help="maximum size of scaled images")
    parser.add_option('', '--hq',
                      action="store_true", dest="hq",
                      help="offer high-quality images as well")
    parser.add_option('', '--mq',
                      action="store_true", dest="mq",
                      help="offer medium-quality images as well")
    parser.add_option('', '--include-flagged',
                      action="store_true", dest="include_flagged", default=False,
                      help="export flagged photos as well")

    options, args = parser.parse_args(args)

    if len(args) != 1:
        sys.stderr.write("Error: Too many arguments.\n")
        sys.stderr.write("usage: %s [OPTIONS]\n" % (args[0],))
        sys.stderr.write("\nTry %s --help for available options.\n" % (args[0],))
        sys.exit(1)

    if options.version:
        print 'original-sync-from-f-spot %s' % VERSION
        print 'Copyright (C) 2006 Andy Wingo.'
        print 'Part of O.R.I.G.I.N.A.L., Jakub Steiner\'s ' \
              'family of web gallery tools.'
        print 'This is free software; see the source for copying conditions.'
        print
        sys.exit(0)

    dbg('options: %r', options)

    ensure_original_in_shotwell()
    ensure_output_path_set()

def get_photos_to_export():
    cur = cxn.cursor()

    sql = 'select id from PhotoTable where 1'
    if not options.resync:
        sql += ' and id not in (select id from original_exports)'
    if not options.include_flagged:
        sql += ' and flags == 0'
    dbg('About to run query: %s', sql)
    cur.execute(sql)
    res = cur.fetchall()
    if res:
        info('Preparing to export %d photos...', len(res))
    else:
        info('Photos are up to date!')
    return [x[0] for x in res]

def get_videos_to_export():
    cur = cxn.cursor()

    sql = 'select id from VideoTable where 1'
    if not options.resync:
        sql += ' and id not in (select id from original_video_export)'
    if not options.include_flagged:
        sql += ' and flags == 0'
    dbg('About to run query: %s', sql)
    cur.execute(sql)
    res = cur.fetchall()
    if res:
        info('Preparing to export %d videos...', len(res))
    else:
        info('Videos are up to date!')
    return [x[0] for x in res]

def jhead(msg, frompath, topath, *options):
    global jhead_checked
    global jhead_available
    if jhead_available:
        dbg(msg)
        args = options + (frompath, topath)
        res = os.spawnlp(os.P_WAIT, 'jhead', 'jhead', *args)
        if not jhead_checked:
            jhead_available = (res == 0)
            if res != 0:
                warn('jhead does not seem to be available; EXIF '
                     'information will not be transferred.')
        elif res != 0:
            warn('Could not transfer EXIF information from %s to %s',
                 frompath, topath)

def transfer_exif(frompath, topath, rotate=False):
    msg = 'calling jhead to transfer exif information from %s to %s' % (frompath,
                                                                        topath)
    jhead(msg, frompath, topath, '-te')

def autorotate(frompath, topath):
    msg = 'rotating %s to %s' % (frompath, topath)
    jhead(msg, frompath, topath, '-autorot')

def last_version(path):
    dirname = os.path.dirname(path)
    basename = os.path.basename(path)
    if " " in basename:
        return True
    basename_id = os.path.splitext(basename)[0] + " "
    photos = [os.path.join(dirname, p) for p in os.listdir(dirname)
              if p.startswith(basename_id)]
    return not photos

def scale_photo(photo_id, tmpdir):

    def scale_bounds(bounds, factor):
        return int(bounds[0] * factor), int(bounds[1] * factor)
    def square_bounds(bounds, dim):
        factor = min([dim/x for x in bounds])
        if factor < 1:
            return scale_bounds(bounds, factor)
        else:
            return bounds
    def mq_bounds(bounds):
        return max(normal_bounds(bounds),
                   scale_bounds(bounds, 0.5))
    def hq_bounds(bounds):
        return bounds
    def normal_bounds(bounds):
        return square_bounds(bounds, options.scaled_size)
    def thumb_bounds(bounds):
        #dims = square_bounds(bounds, options.scaled_size * 0.2)
        dims = square_bounds(bounds, options.scaled_size)
        
        x, y = dims
        if x < y:
            y2 = (y * THUMB_SIZE) / x
            x2 = THUMB_SIZE
            center = (y2 - THUMB_SIZE) / 2
        else:
            y2 = THUMB_SIZE
            x2 = (THUMB_SIZE * x) / y
            center = (x2 - THUMB_SIZE) / 2
        dims = (int(x2), int(y2), int(center))
        return dims
    

    cur = cxn.cursor()
    cur.execute('select filename from PhotoTable where id=%d' % (photo_id,))
    path = cur.fetchone()[0]
    basename = encode(os.path.basename(path))
    dirname = encode(os.path.dirname(path))
    orig_basename = basename

    path = os.path.join(dirname, basename)
    if not last_version(path):
        dbg('%s is not last version', path)
        return []

    info('Preparing to scale %s', path)

    copied_picture = os.path.join(tmpdir, basename)
    basename = orig_basename

    shutil.copyfile(path, copied_picture)
    transfer_exif(path, copied_picture)
    autorotate(path, copied_picture)

    pixbuf = gdk.pixbuf_new_from_file(copied_picture)
    w, h = pixbuf.get_width(), pixbuf.get_height()

    ret = []
    for kind, bounds in (('normal', normal_bounds((w, h))),
                         ('thumb', thumb_bounds((w, h))),
                         ('mq', options.mq and mq_bounds((w, h))),
                         ('hq', options.hq and hq_bounds((w, h)))):
        if bounds:
            parts = basename.split('.')
            parts.insert(len(parts)-1, kind)

            outfile = os.path.join(tmpdir, '.'.join(parts))

            info("Copying %s", outfile)
            if bounds == (w, h):
                # the identity transformation
                dbg('simple copy for %s %s scale', basename, kind)
                copy_file(copied_picture, outfile)
            else:
                dbg('scaling %s to %dx%d for %s', basename, bounds[0],
                    bounds[1], kind)
                copy = pixbuf.scale_simple(bounds[0], bounds[1],
                                           gdk.INTERP_BILINEAR)
                if kind == 'thumb':
                    x, y, center = bounds
                    if x < y:
                        x = 0
                        y = center
                    else:
                        y = 0
                        x = center
                    cropped = copy.subpixbuf(x, y, THUMB_SIZE, THUMB_SIZE)
                    if cropped:
                        copy = cropped
                copy.save(outfile, 'jpeg', {'quality': '90'})
                del copy
            if kind == 'normal':
                transfer_exif(copied_picture, outfile)
            outfile = encode(outfile)
            ret.append((kind, outfile))
        else:
            ret.append((kind, None))
    del pixbuf
    gc.collect()
    os.unlink(copied_picture)
    return ret

def finish(tmpdir):
    dbpath = copy_database(tmpdir)
    fixup_database(dbpath)
    rmrf(tmpdir)
    loop.quit()

def process_video(video_id, tmpdir, remaining):
    import gst

    cur = cxn.cursor()
    cur.execute('select filename,  from VideoTable where id=%d' % (video_id,))
    relpath, = cur.fetchone()
    info('Preparing to scale %s', relpath)

    orig_path = relpath
    orig_filename = os.path.basename(relpath)
    filename = os.path.splitext(orig_filename)[0]

    if relpath.startswith(options.photos_path):
        relpath = relpath[len(options.photos_path):]
    if relpath.startswith('/'):
        relpath = relpath[1:]
    relpath = os.path.dirname(relpath)
    video_directory = os.path.join(output_path, relpath)
    if not os.path.exists(video_directory):
        os.makedirs(video_directory)

    # generate thumbnail
    thumbnail = os.path.join(video_directory, "%s.png" % filename)
    thumbnail_relpath = os.path.join(relpath, "%s.png" % filename)
    os.system("totem-video-thumbnailer %s %s" % (orig_path, thumbnail))

    # transcode to webm
    webm_filename = os.path.join(video_directory, "%s.webm" % filename)
    webm_relpath = os.path.join(relpath, "%s.webm" % filename)
    pipeline_desc = "webmmux name=mux ! filesink location=%s "\
                    "filesrc location=%s ! decodebin2 name=demux "\
                    "demux. ! ffmpegcolorspace ! "\
                    "vp8enc threads=2 ! queue ! "\
                    "progressreport ! mux.video_0 "\
                    "demux. ! audioconvert ! audiorate ! "\
                    "vorbisenc ! queue ! mux.audio_0" % (webm_filename, orig_path)
    pipeline = gst.parse_launch(pipeline_desc)

    def on_eos(*args):
        pipeline.set_state(gst.STATE_NULL)
        req = "insert into original_video_export (id, thumbnail_path, webm_video_path) values (%s, %r, %r)"
        sql = req % (video_id, encode(thumbnail_relpath), encode(webm_relpath))
        dbg('About to run query: %s', sql)
        cur.execute(sql)
        cxn.commit()

        if remaining:
            process_video(remaining[0], tmpdir, remaining[1:])
        else:
            finish(tmpdir)

    def on_error(bus, message):
        print message
        finish(tmpdir)

    bus = pipeline.get_bus()
    bus.add_signal_watch()
    bus.connect("message::eos", on_eos)
    bus.connect("message::error", on_error)

    pipeline.set_state(gst.STATE_PLAYING)

def copy_file(source, destination, force=False):
    if force or not os.path.exists(destination):
        shutil.copy2(source, destination)

def remove_file(path):
    os.remove(path)

def copy_photo(photo_id, tmpdir, scaled):
    cur = cxn.cursor()
    cur.execute('select filename from PhotoTable where id=%d' % photo_id)
    relpath = cur.fetchone()[0]
    if relpath.startswith(options.photos_path):
        relpath = relpath[len(options.photos_path):]
    if relpath[0] == '/':
        relpath = relpath[1:]
    relpath = os.path.dirname(relpath)
    photo_directory = os.path.join(output_path, relpath)
    if not os.path.exists(photo_directory):
        os.makedirs(photo_directory)

    for k, v in scaled:
        if v:
            destination = os.path.join(photo_directory, os.path.basename(v))
            copy_file(v, destination)
            remove_file(v)

    args = (photo_id,) + tuple([x[1] and os.path.join(relpath,
                                                      os.path.basename(x[1]))
                                or '' for x in scaled])
    args2 = tuple([ encode(str(a)) for a in args])
    req = "insert into original_exports (id, normal_relpath,thumb_relpath,mq_relpath, hq_relpath) values (%s, %r, %r, %r, %r)"
    cur.execute(req % args2)
    cxn.commit()

def rmrf(tmpdir):
    for root, dirs, files in os.walk(tmpdir, topdown=False):
        for name in files:
            os.remove(os.path.join(root, name))
        for name in dirs:
            os.rmdir(os.path.join(root, name))
    os.rmdir(tmpdir)

def copy_database(tmpdir):
    # first copy to temp dir, then to output_path
    copy_file(SHOTWELL_DB, tmpdir, force=True)

    db_dir = os.path.join(output_path, "db")
    if not os.path.isdir(db_dir):
        info("Creating %s", db_dir)
        os.makedirs(db_dir)
        htaccess = os.path.join(db_dir, ".htaccess")
        info("Creating %s", htaccess)
        f = open(htaccess, "w")
        f.write("<Files photos.db>\n\tdeny from all\n</Files>\n")
        f.close()

    info("Copying database...")
    db_path = os.path.join(db_dir, "photos.db")
    copy_file(os.path.join(tmpdir, "photo.db"), db_path, force=True)
    return db_path

def make_sane_many_to_many_relations(connection):
    info("Fixing tags relations")
    cur = connection.cursor()

    cur.execute("""CREATE TABLE "TagTable_videos" (
    "id" integer NOT NULL PRIMARY KEY,
    "tag_id" integer NOT NULL,
    "video_id" integer NOT NULL REFERENCES "VideoTable" ("id"),
    UNIQUE ("tag_id", "video_id")
    );""")
    cur.execute("""CREATE TABLE "TagTable_photos" (
    "id" integer NOT NULL PRIMARY KEY,
    "tag_id" integer NOT NULL,
    "photo_id" integer NOT NULL REFERENCES "PhotoTable" ("id"),
    UNIQUE ("tag_id", "photo_id")
    );""")

    connection.commit()
    tv_id = 0
    tp_id = 0
    cur.execute("select id,photo_id_list from TagTable")
    for tag_id, obj_list in cur.fetchall():
        for obj_id in obj_list.split(","):
            if not obj_id:
                continue
            if obj_id.startswith("thumb"):
                table = "TagTable_photos"
                func = shotwell_photo_id_to_db_id
                tp_id += 1
                rel_id = tp_id
                obj_id_name = "photo_id"
            else:
                table = "TagTable_videos"
                func = shotwell_video_id_to_db_id
                tv_id += 1
                rel_id = tv_id
                obj_id_name = "video_id"
            sql = "insert into %s(id,tag_id,%s) values (%d,%d,%d)" % (table, obj_id_name, rel_id,
                                                                      tag_id, func(obj_id))
            cur.execute(sql)
    connection.commit()

def fixup_database(dbpath):
    info("Fixing up photo timestamps...")
    connection = sqlite.connect(dbpath)
    cur = connection.cursor()
    cur.execute('select id,filename from PhotoTable')
    for photo_id, filename in cur.fetchall():
        f = open(filename)
        exif = EXIF.process_file(f)
        f.close()
        timestamp = time.mktime(time.strptime(str(exif["EXIF DateTimeOriginal"]),
                                              "%Y:%m:%d %H:%M:%S"))
        cur.execute('update PhotoTable set timestamp=%d where id=%d' % (timestamp, photo_id))
    make_sane_many_to_many_relations(connection)
    connection.commit()

def process_all(args):
    init(args)
    photos_to_export = get_photos_to_export()
    videos_to_export = get_videos_to_export()
    tmpdir = tempfile.mkdtemp()

    for photo_id in photos_to_export:
        try:
            scaled = scale_photo(photo_id, tmpdir)
        except KeyboardInterrupt:
            raise
        except Exception, e:
            dbg('%s', traceback.format_exc())
            warn('Could not scale photo %d, reason: %s. Skipping.',
                 photo_id, e)
            continue
        if scaled:
            copy_photo(photo_id, tmpdir, scaled)

    if videos_to_export:
        process_video(videos_to_export[0], tmpdir, videos_to_export[1:])
    else:
        finish(tmpdir)

def main(args):
    global loop
    loop = gobject.MainLoop()

    gobject.timeout_add(0, process_all, args)
    loop.run()

if __name__ == '__main__':
    main(sys.argv)
